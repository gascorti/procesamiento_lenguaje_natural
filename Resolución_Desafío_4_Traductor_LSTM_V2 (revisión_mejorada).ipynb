{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfa39F4lsLf3"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## LSTM Traductor\n",
        "Ejemplo basado en [LINK](https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqO0PRcFsPTe"
      },
      "source": [
        "### Datos\n",
        "El objecto es utilizar datos disponibles de Anki de traducciones de texto en diferentes idiomas. Se construirá un modelo traductor seq2seq utilizando encoder-decoder.\\\n",
        "[LINK](https://www.manythings.org/anki/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown --quiet"
      ],
      "metadata": {
        "id": "FGmtK8J19PNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4\n",
        "import numpy as np\n",
        "np.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UPs7izU05woa",
        "outputId": "4762e5bf-717e-4aee-a9bd-d516a2e7caa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.26.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq3YXak9sGHd"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "2M3jVUkDdXPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgYatMIdk_eT",
        "outputId": "ac9a56ce-b0b6-4b6e-9af0-7d63913f69c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torchinfo\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYpIWGaXxfKe",
        "outputId": "49875bd4-3803-4f2a-c479-334e6a4714db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import platform\n",
        "\n",
        "if os.access('torch_helpers.py', os.F_OK) is False:\n",
        "    if platform.system() == 'Windows':\n",
        "        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n",
        "    else:\n",
        "        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"
      ],
      "metadata": {
        "id": "GHFPS5KNxgR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_acc(y_pred, y_test):\n",
        "    # Aseguramos que ambos estén en el mismo device\n",
        "    device = y_pred.device\n",
        "    y_test = y_test.to(device)\n",
        "\n",
        "    y_pred_tag = y_pred.data.max(dim=-1, keepdim=True)[1]\n",
        "    y_test_tag = y_test.data.max(dim=-1, keepdim=True)[1]\n",
        "\n",
        "    batch_size = y_pred_tag.shape[0]\n",
        "    # IMPORTANTE: crear batch_acc en el mismo device\n",
        "    batch_acc = torch.zeros(batch_size, device=device)\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        correct_results_sum = (y_pred_tag[b] == y_test_tag[b]).sum().float()\n",
        "        batch_acc[b] = correct_results_sum / y_pred_tag[b].shape[0]\n",
        "\n",
        "    correct_results_sum = batch_acc.sum().float()\n",
        "    acc = correct_results_sum / batch_size\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n",
        "    # Listas para graficar resultados\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    valid_loss = []\n",
        "    valid_accuracy = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_train_accuracy = 0.0\n",
        "\n",
        "        # ---------- ENTRENAMIENTO ----------\n",
        "        for train_encoder_input, train_decoder_input, train_target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Enviar TODO al mismo device\n",
        "            train_encoder_input = train_encoder_input.to(device)\n",
        "            train_decoder_input = train_decoder_input.to(device)\n",
        "            train_target = train_target.to(device)\n",
        "\n",
        "            output = model(train_encoder_input, train_decoder_input)\n",
        "\n",
        "            # sequence loss (por token)\n",
        "            loss = 0\n",
        "            for t in range(train_decoder_input.shape[1]):\n",
        "                target_t = train_target[:, t, :].argmax(dim=1)  # (batch,)\n",
        "                loss += criterion(output[:, t, :], target_t)\n",
        "\n",
        "            epoch_train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            accuracy = sequence_acc(output, train_target)\n",
        "            epoch_train_accuracy += accuracy.item()\n",
        "\n",
        "        epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_loss.append(epoch_train_loss)\n",
        "\n",
        "        epoch_train_accuracy = epoch_train_accuracy / len(train_loader)\n",
        "        train_accuracy.append(epoch_train_accuracy)\n",
        "\n",
        "        # ---------- VALIDACIÓN ----------\n",
        "        with torch.no_grad():\n",
        "            valid_encoder_input, valid_decoder_input, valid_target = next(iter(valid_loader))\n",
        "\n",
        "            valid_encoder_input = valid_encoder_input.to(device)\n",
        "            valid_decoder_input = valid_decoder_input.to(device)\n",
        "            valid_target = valid_target.to(device)\n",
        "\n",
        "            output = model(valid_encoder_input, valid_decoder_input)\n",
        "\n",
        "            epoch_valid_loss = 0\n",
        "            for t in range(valid_decoder_input.shape[1]):\n",
        "                target_t = valid_target[:, t, :].argmax(dim=1)\n",
        "                epoch_valid_loss += criterion(output[:, t, :], target_t)\n",
        "\n",
        "            epoch_valid_loss = epoch_valid_loss.item()\n",
        "            valid_loss.append(epoch_valid_loss)\n",
        "\n",
        "            epoch_valid_accuracy = sequence_acc(output, valid_target).item()\n",
        "            valid_accuracy.append(epoch_valid_accuracy)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} - \"\n",
        "              f\"Train loss {epoch_train_loss:.3f} - \"\n",
        "              f\"Train accuracy {epoch_train_accuracy:.3f} - \"\n",
        "              f\"Valid Loss {epoch_valid_loss:.3f} - \"\n",
        "              f\"Valid accuracy {epoch_valid_accuracy:.3f}\")\n",
        "\n",
        "    history = {\n",
        "        \"loss\": train_loss,\n",
        "        \"accuracy\": train_accuracy,\n",
        "        \"val_loss\": valid_loss,\n",
        "        \"val_accuracy\": valid_accuracy,\n",
        "    }\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "tP-fbmHUgbtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 - Datos"
      ],
      "metadata": {
        "id": "5BFiCH8nxoIY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHNkUaPp6aYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57774716-c27e-4994-d6f2-a69043d1a1b6"
      },
      "source": [
        "# Descargar la carpeta de dataset\n",
        "import os\n",
        "import gdown\n",
        "if os.access('spa-eng', os.F_OK) is False:\n",
        "    if os.access('simpsons_dataset.zip', os.F_OK) is False:\n",
        "        url = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip'\n",
        "        output = 'spa-eng.zip'\n",
        "        gdown.download(url, output, quiet=False)\n",
        "    !unzip -q spa-eng.zip\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El dataset ya se encuentra descargado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9aNLZBDtA5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a33c80-5760-4b66-b4d0-47f0d07cbce3"
      },
      "source": [
        "# dataset_file\n",
        "\n",
        "text_file = \"./spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "# Por limitaciones de RAM no se leen todas las filas\n",
        "MAX_NUM_SENTENCES = 20000\n",
        "\n",
        "# Mezclar el dataset, forzar semilla siempre igual\n",
        "np.random.seed([40])\n",
        "np.random.shuffle(lines)\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "count = 0\n",
        "\n",
        "for line in lines:\n",
        "    count += 1\n",
        "    if count > MAX_NUM_SENTENCES:\n",
        "        break\n",
        "\n",
        "    if '\\t' not in line:\n",
        "        continue\n",
        "\n",
        "    # Input sentence --> eng\n",
        "    # output --> spa\n",
        "    input_sentence, output = line.rstrip().split('\\t')\n",
        "\n",
        "    # output sentence (decoder_output) tiene <eos>\n",
        "    output_sentence = output + ' <eos>'\n",
        "    # output sentence input (decoder_input) tiene <sos>\n",
        "    output_sentence_input = '<sos> ' + output\n",
        "\n",
        "    input_sentences.append(input_sentence)\n",
        "    output_sentences.append(output_sentence)\n",
        "    output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "print(\"Cantidad de rows disponibles:\", len(lines))\n",
        "print(\"Cantidad de rows utilizadas:\", len(input_sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de rows disponibles: 118964\n",
            "Cantidad de rows utilizadas: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93IGMKFb73q7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3649a8c-538e-4ff5-9816-dd410c48473b"
      },
      "source": [
        "input_sentences[0], output_sentences[0], output_sentences_inputs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('A deal is a deal.',\n",
              " 'Un trato es un trato. <eos>',\n",
              " '<sos> Un trato es un trato.')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P-ynUNP5xp6"
      },
      "source": [
        "### 2 - Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WAZGOTfGyha"
      },
      "source": [
        "# Definir el tamaño máximo del vocabulario\n",
        "MAX_VOCAB_SIZE = 8000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF1W6peoFGXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d9f1e9-efa5-4882-9084-4b2dc9462134"
      },
      "source": [
        "# Longitudes máximas “deseadas”\n",
        "MAX_INPUT_LEN = 26   # antes 16\n",
        "MAX_OUT_LEN   = 28   # antes 18\n",
        "\n",
        "from torch_helpers import Tokenizer\n",
        "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "input_tokenizer.fit_on_texts(input_sentences)\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
        "\n",
        "word2idx_inputs = input_tokenizer.word_index\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n",
        "\n",
        "max_input_len = min(MAX_INPUT_LEN, max(len(sen) for sen in input_integer_seq))\n",
        "print(\"Longitud máxima entrada usada:\", max_input_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras en el vocabulario: 6820\n",
            "Longitud máxima entrada usada: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBzdKiTVIBYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f696f06c-12ff-4a3c-f992-27534ff9a0b6"
      },
      "source": [
        "# A los filtros de símbolos del Tokenizer agregamos el \"¿\",\n",
        "# sacamos los \"<>\" para que no afectar nuestros tokens\n",
        "output_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n",
        "output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)\n",
        "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
        "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "word2idx_outputs = output_tokenizer.word_index\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n",
        "\n",
        "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE)\n",
        "idx2word_target = {idx: word for word, idx in word2idx_outputs.items()}\n",
        "\n",
        "max_out_len   = min(MAX_OUT_LEN,   max(len(sen) for sen in output_integer_seq))\n",
        "print(\"Longitud máxima salida usada:\", max_out_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras en el vocabulario: 11262\n",
            "Longitud máxima salida usada: 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqb8ZJ4sJHgv"
      },
      "source": [
        "Como era de esperarse, las sentencias en castellano son más largas que en inglés, y lo mismo sucede con su vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgLC706EQx3p"
      },
      "source": [
        "max_input_len = 16\n",
        "max_out_len = 18"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Ob4hAWJkcv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf59f6f-2498-4b26-c76f-dd6acd837627"
      },
      "source": [
        "from torch_helpers import pad_sequences\n",
        "print(\"Cantidad de rows del dataset:\", len(input_integer_seq))\n",
        "\n",
        "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
        "print(\"encoder_input_sequences shape:\", encoder_input_sequences.shape)\n",
        "\n",
        "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"decoder_input_sequences shape:\", decoder_input_sequences.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de rows del dataset: 20000\n",
            "encoder_input_sequences shape: (20000, 16)\n",
            "decoder_input_sequences shape: (20000, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"decoder_output_sequences shape:\", decoder_output_sequences.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VySR1pzx9UG",
        "outputId": "df6a4692-604b-42d4-bce7-68ff0c5b0266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder_output_sequences shape: (20000, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.from_numpy(decoder_output_sequences).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANTOqJ0WWw-q",
        "outputId": "6f390e16-632d-4eb5-c422-8f3200555fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20000, 18])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, encoder_input, decoder_input, decoder_output):\n",
        "        # Convertir los arrays de numpy a tensores.\n",
        "        # pytorch espera en general entradas 32bits\n",
        "        self.encoder_inputs = torch.from_numpy(encoder_input.astype(np.int32))\n",
        "        self.decoder_inputs = torch.from_numpy(decoder_input.astype(np.int32))\n",
        "        # Transformar los datos a oneHotEncoding\n",
        "        # la loss function esperan la salida float\n",
        "        self.decoder_outputs = F.one_hot(torch.from_numpy(decoder_output).to(torch.int64), num_classes=num_words_output).float()\n",
        "\n",
        "        self.len = self.decoder_outputs.shape[0]\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.encoder_inputs[index], self.decoder_inputs[index], self.decoder_outputs[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "data_set = Data(encoder_input_sequences, decoder_input_sequences, decoder_output_sequences)\n",
        "\n",
        "encoder_input_size = data_set.encoder_inputs.shape[1]\n",
        "print(\"encoder_input_size:\", encoder_input_size)\n",
        "\n",
        "decoder_input_size = data_set.decoder_inputs.shape[1]\n",
        "print(\"decoder_input_size:\", decoder_input_size)\n",
        "\n",
        "output_dim = data_set.decoder_outputs.shape[2]\n",
        "print(\"Output dim\", output_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD0bpM32yWfB",
        "outputId": "71e034b1-823a-4b38-f43c-d0fb2ab391e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_input_size: 16\n",
            "decoder_input_size: 18\n",
            "Output dim 8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "valid_set_size = int(data_set.len * 0.2)\n",
        "train_set_size = data_set.len - valid_set_size\n",
        "\n",
        "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
        "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n",
        "\n",
        "print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n",
        "print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUDPZeuAU1RI",
        "outputId": "c8e5ea48-985f-4bea-ae18-df52d2e6764d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del conjunto de entrenamiento: 16000\n",
            "Tamaño del conjunto de validacion: 4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJIsLBbj6rg"
      },
      "source": [
        "### 3 - Preparar los embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OcT-DLzkHS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0fc530f-7510-4d1f-a84d-82dad2874761"
      },
      "source": [
        "import os\n",
        "import gdown\n",
        "if os.access('gloveembedding.pkl', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download'\n",
        "    output = 'gloveembedding.pkl'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"Los embeddings gloveembedding.pkl ya están descargados\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download\n",
            "From (redirected): https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download&confirm=t&uuid=f79de2aa-a2dc-4be2-a8c5-c13d361772ab\n",
            "To: /content/gloveembedding.pkl\n",
            "100%|██████████| 525M/525M [00:10<00:00, 49.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgqtV8GpkSc8"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "import pickle\n",
        "\n",
        "class WordsEmbeddings(object):\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    def __init__(self):\n",
        "        # load the embeddings\n",
        "        words_embedding_pkl = Path(self.PKL_PATH)\n",
        "        if not words_embedding_pkl.is_file():\n",
        "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
        "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
        "            embeddings = self.convert_model_to_pickle()\n",
        "        else:\n",
        "            embeddings = self.load_model_from_pickle()\n",
        "        self.embeddings = embeddings\n",
        "        # build the vocabulary hashmap\n",
        "        index = np.arange(self.embeddings.shape[0])\n",
        "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
        "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
        "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
        "\n",
        "    def get_words_embeddings(self, words):\n",
        "        words_idxs = self.words2idxs(words)\n",
        "        return self.embeddings[words_idxs]['embedding']\n",
        "\n",
        "    def words2idxs(self, words):\n",
        "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
        "\n",
        "    def idxs2words(self, idxs):\n",
        "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
        "\n",
        "    def load_model_from_pickle(self):\n",
        "        self.logger.debug(\n",
        "            'loading words embeddings from pickle {}'.format(\n",
        "                self.PKL_PATH\n",
        "            )\n",
        "        )\n",
        "        max_bytes = 2**28 - 1 # 256MB\n",
        "        bytes_in = bytearray(0)\n",
        "        input_size = os.path.getsize(self.PKL_PATH)\n",
        "        with open(self.PKL_PATH, 'rb') as f_in:\n",
        "            for _ in range(0, input_size, max_bytes):\n",
        "                bytes_in += f_in.read(max_bytes)\n",
        "        embeddings = pickle.loads(bytes_in)\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "    def convert_model_to_pickle(self):\n",
        "        # create a numpy strctured array:\n",
        "        # word     embedding\n",
        "        # U50      np.float32[]\n",
        "        # word_1   a, b, c\n",
        "        # word_2   d, e, f\n",
        "        # ...\n",
        "        # word_n   g, h, i\n",
        "        self.logger.debug(\n",
        "            'converting and loading words embeddings from text file {}'.format(\n",
        "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
        "            )\n",
        "        )\n",
        "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
        "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
        "        structure = np.dtype(structure)\n",
        "        # load numpy array from disk using a generator\n",
        "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
        "            embeddings_gen = (\n",
        "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
        "                if len(line.split()[1:]) == self.N_FEATURES\n",
        "            )\n",
        "            embeddings = np.fromiter(embeddings_gen, structure)\n",
        "        # add a null embedding\n",
        "        null_embedding = np.array(\n",
        "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
        "            dtype=structure\n",
        "        )\n",
        "        embeddings = np.concatenate([embeddings, null_embedding])\n",
        "        # dump numpy array to disk using pickle\n",
        "        max_bytes = 2**28 - 1 # # 256MB\n",
        "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(self.PKL_PATH, 'wb') as f_out:\n",
        "            for idx in range(0, len(bytes_out), max_bytes):\n",
        "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class GloveEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
        "    PKL_PATH = 'gloveembedding.pkl'\n",
        "    N_FEATURES = 50\n",
        "    WORD_MAX_SIZE = 60\n",
        "\n",
        "class FasttextEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n",
        "    PKL_PATH = 'fasttext.pkl'\n",
        "    N_FEATURES = 300\n",
        "    WORD_MAX_SIZE = 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementación beam search"
      ],
      "metadata": {
        "id": "QTTdYfNPgi4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def beam_search_decode(model, encoder_seq, beam_width=3, max_len=None):\n",
        "    \"\"\"\n",
        "    Decodificación con beam search.\n",
        "    encoder_seq: np.array (1, max_input_len) o tensor\n",
        "    beam_width: tamaño del haz\n",
        "    max_len: longitud máxima de secuencia generada\n",
        "    \"\"\"\n",
        "    if max_len is None:\n",
        "        max_len = max_out_len\n",
        "\n",
        "    # A tensor + device\n",
        "    if isinstance(encoder_seq, np.ndarray):\n",
        "        encoder_seq = torch.from_numpy(encoder_seq.astype(np.int64))\n",
        "    encoder_seq = encoder_seq.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Paso por el encoder\n",
        "        prev_state = model.encoder(encoder_seq)\n",
        "\n",
        "        sos = word2idx_outputs['<sos>']\n",
        "        eos = word2idx_outputs['<eos>']\n",
        "\n",
        "        # Cada beam = (log_prob_acumulada, [tokens_ids], prev_state, last_token_tensor)\n",
        "        start_token = torch.tensor([[sos]], device=device)\n",
        "        beams = [(0.0, [sos], prev_state, start_token)]\n",
        "        completed = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            new_beams = []\n",
        "\n",
        "            for log_p, seq, state, last_token in beams:\n",
        "                # Si ya llegó a <eos>, lo pasamos a completados\n",
        "                if seq[-1] == eos:\n",
        "                    completed.append((log_p, seq))\n",
        "                    continue\n",
        "\n",
        "                # Decoder un paso\n",
        "                out, new_state = model.decoder(last_token, state)  # out: (1, vocab)\n",
        "                # Trabajamos en log-prob\n",
        "                log_probs = torch.log(out.squeeze(0) + 1e-12)      # (vocab,)\n",
        "                topk_log_probs, topk_idx = torch.topk(log_probs, beam_width)\n",
        "\n",
        "                for k in range(beam_width):\n",
        "                    idx = int(topk_idx[k].item())\n",
        "                    new_log_p = log_p + float(topk_log_probs[k].item())\n",
        "                    new_seq = seq + [idx]\n",
        "                    new_token = torch.tensor([[idx]], device=device)\n",
        "                    new_beams.append((new_log_p, new_seq, new_state, new_token))\n",
        "\n",
        "            if not new_beams:\n",
        "                break\n",
        "\n",
        "            # Nos quedamos con los mejores beam_width\n",
        "            new_beams.sort(key=lambda x: x[0], reverse=True)\n",
        "            beams = new_beams[:beam_width]\n",
        "\n",
        "        # Añadimos lo que haya quedado vivo\n",
        "        completed += beams\n",
        "        completed.sort(key=lambda x: x[0], reverse=True)\n",
        "        best_seq = completed[0][1]\n",
        "\n",
        "        # Convertimos ids → palabras (salteando <sos> y <eos>)\n",
        "        sos = word2idx_outputs['<sos>']\n",
        "        eos = word2idx_outputs['<eos>']\n",
        "\n",
        "        words = []\n",
        "        for idx in best_seq:\n",
        "            if idx in (sos, eos):\n",
        "                continue\n",
        "            if idx > 0 and idx in idx2word_target:\n",
        "                words.append(idx2word_target[idx])\n",
        "\n",
        "        return ' '.join(words)\n"
      ],
      "metadata": {
        "id": "8ArO5svqgdLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mosj2-x-kXBK"
      },
      "source": [
        "# Por una cuestion de RAM se utilizará los embeddings de Glove de dimension 50\n",
        "model_embeddings = GloveEmbeddings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9FS8ca1ke_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b5651c-fa0e-45d2-b6fe-6745c329c34b"
      },
      "source": [
        "# Crear la Embedding matrix de las secuencias\n",
        "# en ingles\n",
        "\n",
        "print('preparing embedding matrix...')\n",
        "embed_dim = model_embeddings.N_FEATURES\n",
        "words_not_found = []\n",
        "\n",
        "# word_index provieen del tokenizer\n",
        "\n",
        "nb_words = min(MAX_VOCAB_SIZE, len(word2idx_inputs)) # vocab_size\n",
        "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "for word, i in word2idx_inputs.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        words_not_found.append(word)\n",
        "\n",
        "print('number of null word embeddings:', np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preparing embedding matrix...\n",
            "number of null word embeddings: 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q3U_WmEYRdH",
        "outputId": "1c56d7cb-c5a6-4adc-919d-a00c1d83620d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6820"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpzJODHBlAtE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97eea2d5-19a2-4a9e-ffda-5c9cfdf631fb"
      },
      "source": [
        "# Dimensión de los embeddings de la secuencia en ingles\n",
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6820, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vKbhjtIwPgM"
      },
      "source": [
        "### 4 - Entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fm3HCLMPSG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f85e941c-20b7-41ce-fd66-7efcf8fd5316"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, lstm_size=128, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.lstm_size = lstm_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding_dim = embed_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = True  # congelar embeddings\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.lstm_size,\n",
        "            batch_first=True,\n",
        "            num_layers=self.num_layers\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        lstm_output, (ht, ct) = self.lstm(out)\n",
        "        return (ht, ct)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, output_dim, lstm_size=128, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.lstm_size = lstm_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding_dim = embed_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.lstm_size,\n",
        "            batch_first=True,\n",
        "            num_layers=self.num_layers\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=self.lstm_size,\n",
        "                             out_features=self.output_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        out = self.embedding(x)\n",
        "        lstm_output, (ht, ct) = self.lstm(out, prev_state)\n",
        "        out = self.softmax(self.fc1(lstm_output[:, -1, :]))  # último paso temporal\n",
        "        return out, (ht, ct)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        assert encoder.lstm_size == decoder.lstm_size, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.num_layers == decoder.num_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, encoder_input_data, decoder_input_data):\n",
        "\n",
        "        # Aseguramos que TODO esté en el mismo device que el modelo\n",
        "        device = next(self.parameters()).device\n",
        "        encoder_input_data = encoder_input_data.to(device)\n",
        "        decoder_input_data = decoder_input_data.to(device)\n",
        "\n",
        "        # Encoder\n",
        "        encoder_last_state = self.encoder(encoder_input_data)\n",
        "\n",
        "        # Decoder (teacher forcing)\n",
        "        outputs = []\n",
        "        prev_state = encoder_last_state\n",
        "\n",
        "        for t in range(decoder_input_data.size(1)):\n",
        "            # tomamos el token t\n",
        "            dec_input_t = decoder_input_data[:, t].unsqueeze(1)\n",
        "            out, prev_state = self.decoder(dec_input_t, prev_state)\n",
        "            outputs.append(out.unsqueeze(1))\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "encoder = Encoder(vocab_size=nb_words)\n",
        "if cuda: encoder.cuda()\n",
        "# decoder --> vocab_size == output_dim --> porque recibe y devuelve palabras en el mismo vocabulario\n",
        "decoder = Decoder(vocab_size=num_words_output, output_dim=num_words_output)\n",
        "if cuda: decoder.cuda()\n",
        "\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "if cuda: model.cuda()\n",
        "\n",
        "# Crear el optimizador la una función de error\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Para clasificación multi categórica\n",
        "\n",
        "summary(model, input_data=(data_set[0:1][0], data_set[0:1][1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Seq2Seq                                  [1, 18, 8000]             --\n",
              "├─Encoder: 1-1                           [1, 1, 128]               --\n",
              "│    └─Embedding: 2-1                    [1, 16, 50]               341,000\n",
              "│    └─LSTM: 2-2                         [1, 16, 128]              92,160\n",
              "├─Decoder: 1-2                           [1, 8000]                 --\n",
              "│    └─Embedding: 2-3                    [1, 1, 50]                400,000\n",
              "│    └─LSTM: 2-4                         [1, 1, 128]               92,160\n",
              "│    └─Linear: 2-5                       [1, 8000]                 1,032,000\n",
              "│    └─Softmax: 2-6                      [1, 8000]                 --\n",
              "├─Decoder: 1-3                           [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-7                    [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-8                         [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-9                       [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-10                     [1, 8000]                 --\n",
              "├─Decoder: 1-4                           [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-11                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-12                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-13                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-14                     [1, 8000]                 --\n",
              "├─Decoder: 1-5                           [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-15                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-16                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-17                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-18                     [1, 8000]                 --\n",
              "├─Decoder: 1-6                           [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-19                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-20                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-21                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-22                     [1, 8000]                 --\n",
              "├─Decoder: 1-7                           [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-23                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-24                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-25                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-26                     [1, 8000]                 --\n",
              "├─Decoder: 1-8                           [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-27                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-28                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-29                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-30                     [1, 8000]                 --\n",
              "├─Decoder: 1-9                           [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-31                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-32                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-33                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-34                     [1, 8000]                 --\n",
              "├─Decoder: 1-10                          [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-35                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-36                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-37                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-38                     [1, 8000]                 --\n",
              "├─Decoder: 1-11                          [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-39                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-40                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-41                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-42                     [1, 8000]                 --\n",
              "├─Decoder: 1-12                          [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-43                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-44                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-45                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-46                     [1, 8000]                 --\n",
              "├─Decoder: 1-13                          [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-47                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-48                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-49                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-50                     [1, 8000]                 --\n",
              "├─Decoder: 1-14                          [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-51                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-52                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-53                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-54                     [1, 8000]                 --\n",
              "├─Decoder: 1-15                          [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-55                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-56                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-57                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-58                     [1, 8000]                 --\n",
              "├─Decoder: 1-16                          [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-59                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-60                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-61                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-62                     [1, 8000]                 --\n",
              "├─Decoder: 1-17                          [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-63                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-64                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-65                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-66                     [1, 8000]                 --\n",
              "├─Decoder: 1-18                          [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-67                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-68                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-69                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-70                     [1, 8000]                 --\n",
              "├─Decoder: 1-19                          [1, 8000]                 (recursive)\n",
              "│    └─Embedding: 2-71                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-72                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-73                      [1, 8000]                 (recursive)\n",
              "│    └─Softmax: 2-74                     [1, 8000]                 --\n",
              "==========================================================================================\n",
              "Total params: 1,957,320\n",
              "Trainable params: 1,957,320\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 29.25\n",
              "==========================================================================================\n",
              "Input size (MB): 2.72\n",
              "Forward/backward pass size (MB): 1.20\n",
              "Params size (MB): 7.83\n",
              "Estimated Total Size (MB): 11.75\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_sizes = [256]\n",
        "histories = {}\n",
        "\n",
        "for h in hidden_sizes:\n",
        "    print(f\"\\n>>> Entrenando modelo con {h} neuronas LSTM\")\n",
        "\n",
        "    encoder = Encoder(vocab_size=nb_words, lstm_size=h)\n",
        "    decoder = Decoder(vocab_size=num_words_output,\n",
        "                      output_dim=num_words_output,\n",
        "                      lstm_size=h)\n",
        "\n",
        "    if cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    history = train(\n",
        "        model,\n",
        "        train_loader,\n",
        "        valid_loader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        epochs=35\n",
        "    )\n",
        "\n",
        "    histories[h] = history\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GdyzFb7b4oti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b516877e-17cc-4e7b-fd6a-5b29a5512cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Entrenando modelo con 256 neuronas LSTM\n",
            "Epoch: 1/35 - Train loss 151.409 - Train accuracy 0.609 - Valid Loss 150.181 - Valid accuracy 0.644\n",
            "Epoch: 2/35 - Train loss 150.721 - Train accuracy 0.614 - Valid Loss 150.180 - Valid accuracy 0.644\n",
            "Epoch: 3/35 - Train loss 150.720 - Train accuracy 0.614 - Valid Loss 150.180 - Valid accuracy 0.644\n",
            "Epoch: 4/35 - Train loss 150.720 - Train accuracy 0.614 - Valid Loss 150.180 - Valid accuracy 0.644\n",
            "Epoch: 5/35 - Train loss 150.720 - Train accuracy 0.614 - Valid Loss 150.180 - Valid accuracy 0.644\n",
            "Epoch: 6/35 - Train loss 150.720 - Train accuracy 0.614 - Valid Loss 150.180 - Valid accuracy 0.644\n",
            "Epoch: 7/35 - Train loss 150.720 - Train accuracy 0.614 - Valid Loss 150.180 - Valid accuracy 0.644\n",
            "Epoch: 8/35 - Train loss 150.720 - Train accuracy 0.614 - Valid Loss 150.180 - Valid accuracy 0.644\n",
            "Epoch: 9/35 - Train loss 150.720 - Train accuracy 0.614 - Valid Loss 150.180 - Valid accuracy 0.644\n",
            "Epoch: 10/35 - Train loss 150.660 - Train accuracy 0.617 - Valid Loss 150.032 - Valid accuracy 0.653\n",
            "Epoch: 11/35 - Train loss 150.574 - Train accuracy 0.622 - Valid Loss 150.024 - Valid accuracy 0.653\n",
            "Epoch: 12/35 - Train loss 150.489 - Train accuracy 0.627 - Valid Loss 149.931 - Valid accuracy 0.658\n",
            "Epoch: 13/35 - Train loss 149.691 - Train accuracy 0.674 - Valid Loss 148.916 - Valid accuracy 0.717\n",
            "Epoch: 14/35 - Train loss 149.439 - Train accuracy 0.686 - Valid Loss 148.907 - Valid accuracy 0.715\n",
            "Epoch: 15/35 - Train loss 149.412 - Train accuracy 0.687 - Valid Loss 148.893 - Valid accuracy 0.715\n",
            "Epoch: 16/35 - Train loss 149.401 - Train accuracy 0.688 - Valid Loss 148.901 - Valid accuracy 0.715\n",
            "Epoch: 17/35 - Train loss 149.398 - Train accuracy 0.688 - Valid Loss 148.897 - Valid accuracy 0.715\n",
            "Epoch: 18/35 - Train loss 149.395 - Train accuracy 0.688 - Valid Loss 148.904 - Valid accuracy 0.715\n",
            "Epoch: 19/35 - Train loss 149.392 - Train accuracy 0.688 - Valid Loss 148.879 - Valid accuracy 0.717\n",
            "Epoch: 20/35 - Train loss 149.391 - Train accuracy 0.688 - Valid Loss 148.905 - Valid accuracy 0.715\n",
            "Epoch: 21/35 - Train loss 149.355 - Train accuracy 0.690 - Valid Loss 148.820 - Valid accuracy 0.720\n",
            "Epoch: 22/35 - Train loss 149.305 - Train accuracy 0.693 - Valid Loss 148.785 - Valid accuracy 0.722\n",
            "Epoch: 23/35 - Train loss 149.283 - Train accuracy 0.694 - Valid Loss 148.790 - Valid accuracy 0.720\n",
            "Epoch: 24/35 - Train loss 149.257 - Train accuracy 0.696 - Valid Loss 148.727 - Valid accuracy 0.726\n",
            "Epoch: 25/35 - Train loss 149.209 - Train accuracy 0.698 - Valid Loss 148.703 - Valid accuracy 0.727\n",
            "Epoch: 26/35 - Train loss 149.188 - Train accuracy 0.700 - Valid Loss 148.716 - Valid accuracy 0.724\n",
            "Epoch: 27/35 - Train loss 149.144 - Train accuracy 0.702 - Valid Loss 148.625 - Valid accuracy 0.731\n",
            "Epoch: 28/35 - Train loss 149.068 - Train accuracy 0.707 - Valid Loss 148.565 - Valid accuracy 0.736\n",
            "Epoch: 29/35 - Train loss 149.016 - Train accuracy 0.710 - Valid Loss 148.529 - Valid accuracy 0.738\n",
            "Epoch: 30/35 - Train loss 148.949 - Train accuracy 0.714 - Valid Loss 148.457 - Valid accuracy 0.743\n",
            "Epoch: 31/35 - Train loss 148.903 - Train accuracy 0.716 - Valid Loss 148.410 - Valid accuracy 0.745\n",
            "Epoch: 32/35 - Train loss 148.874 - Train accuracy 0.718 - Valid Loss 148.410 - Valid accuracy 0.745\n",
            "Epoch: 33/35 - Train loss 148.848 - Train accuracy 0.719 - Valid Loss 148.500 - Valid accuracy 0.738\n",
            "Epoch: 34/35 - Train loss 148.824 - Train accuracy 0.720 - Valid Loss 148.508 - Valid accuracy 0.738\n",
            "Epoch: 35/35 - Train loss 148.802 - Train accuracy 0.721 - Valid Loss 148.401 - Valid accuracy 0.745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = range(1, len(next(iter(histories.values()))['accuracy']) + 1)\n",
        "\n",
        "for h, hist in histories.items():\n",
        "    sns.lineplot(x=epoch_count, y=hist['val_accuracy'], label=f'LSTM {h}')\n",
        "\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Accuracy validación')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p-I-s4Fd4w8z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "ebc96d50-66e8-401a-af44-96142eaf26f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mostrar 5 ejemplos de traducciones generadas**"
      ],
      "metadata": {
        "id": "u8KE30LX43vO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(input_seq, mode=\"greedy\", beam_width=3):\n",
        "    \"\"\"\n",
        "    input_seq: np.array (1, max_input_len) o tensor\n",
        "    mode: \"greedy\" o \"beam\"\n",
        "    \"\"\"\n",
        "    if mode == \"beam\":\n",
        "        return beam_search_decode(model, input_seq, beam_width=beam_width)\n",
        "\n",
        "    # --- modo GREEDY (el que ya tenías) ---\n",
        "    if isinstance(input_seq, np.ndarray):\n",
        "        encoder_seq = torch.from_numpy(input_seq.astype(np.int64))\n",
        "    else:\n",
        "        encoder_seq = input_seq\n",
        "\n",
        "    encoder_seq = encoder_seq.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prev_state = model.encoder(encoder_seq)\n",
        "\n",
        "        target_seq = np.zeros((1, 1), dtype='int32')\n",
        "        target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "        target_seq_tensor = torch.from_numpy(target_seq).to(device)\n",
        "\n",
        "        eos = word2idx_outputs['<eos>']\n",
        "        output_sentence = []\n",
        "\n",
        "        for _ in range(max_out_len):\n",
        "            output, prev_state = model.decoder(target_seq_tensor, prev_state)\n",
        "            top1 = output.argmax(1).view(-1, 1)\n",
        "            idx = int(top1.item())\n",
        "\n",
        "            if idx == eos:\n",
        "                break\n",
        "\n",
        "            if idx > 0 and idx in idx2word_target:\n",
        "                word = idx2word_target[idx]\n",
        "                output_sentence.append(word)\n",
        "\n",
        "            target_seq_tensor = top1\n",
        "\n",
        "    return ' '.join(output_sentence)\n"
      ],
      "metadata": {
        "id": "hmmCUIXOgy6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizar los 5 ejemplos (aleatorios)"
      ],
      "metadata": {
        "id": "omSsAVhW5G-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_random_examples(n=5, mode=\"greedy\", beam_width=3):\n",
        "    for _ in range(n):\n",
        "        i = np.random.choice(len(input_sentences))\n",
        "        input_text = input_sentences[i]\n",
        "        target_text = output_sentences[i]\n",
        "\n",
        "        input_seq = encoder_input_sequences[i:i+1]\n",
        "\n",
        "        translation = translate_sentence(input_seq, mode=mode, beam_width=beam_width)\n",
        "\n",
        "        print('-' * 60)\n",
        "        print(f'Input      : {input_text}')\n",
        "        print(f'Target     : {target_text}')\n",
        "        print(f'Predicción : {translation}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AAOgq2pMg36P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "show_random_examples(5, mode=\"greedy\")\n",
        "show_random_examples(5, mode=\"beam\", beam_width=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VInee4XB8hs6",
        "outputId": "fe331763-ab3d-4e51-b4ac-a985169521ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "Input      : Your work has greatly improved.\n",
            "Target     : Tu trabajo ha mejorado notablemente. <eos>\n",
            "Predicción : es un\n",
            "------------------------------------------------------------\n",
            "Input      : Tom is naked.\n",
            "Target     : Tom anda encuerado. <eos>\n",
            "Predicción : tom es un\n",
            "------------------------------------------------------------\n",
            "Input      : I can't hear you.\n",
            "Target     : No puedo oírte. <eos>\n",
            "Predicción : no es un\n",
            "------------------------------------------------------------\n",
            "Input      : She seemed to be suffering a heart attack.\n",
            "Target     : Ella parecía estar sufriendo un ataque al corazón. <eos>\n",
            "Predicción : a la de la\n",
            "------------------------------------------------------------\n",
            "Input      : Don't pretend what you don't feel.\n",
            "Target     : ¡No finja usted lo que no siente! <eos>\n",
            "Predicción : no es que no es un\n",
            "------------------------------------------------------------\n",
            "Input      : He is in good physical condition.\n",
            "Target     : Él está en buena condición física. <eos>\n",
            "Predicción : es un de la\n",
            "------------------------------------------------------------\n",
            "Input      : I'll explain it to you later on.\n",
            "Target     : Más adelante se lo explicaré. <eos>\n",
            "Predicción : a la de la\n",
            "------------------------------------------------------------\n",
            "Input      : I'm sorry I couldn't write to you sooner.\n",
            "Target     : Lamento no haber podido escribirte antes. <eos>\n",
            "Predicción : no es que la\n",
            "------------------------------------------------------------\n",
            "Input      : You don't have a fever.\n",
            "Target     : No tienes fiebre. <eos>\n",
            "Predicción : no es un\n",
            "------------------------------------------------------------\n",
            "Input      : She can skate.\n",
            "Target     : Ella puede patinar. <eos>\n",
            "Predicción : a la\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusiones**"
      ],
      "metadata": {
        "id": "hbBjxTG4pCXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Más allá que de haber implementado greedy y beam search (utiluzando LSTM de 256 neuronas) no hubo buen resultado de traducción. Inclusive con una ligeramente mejor accuracy resultando en un mejor entrenamiento con las palabras, no mejoró la capacidad de predecir en base al contexto. Como trabajo futuro se propone la implementación de algun modelo con atención.  "
      ],
      "metadata": {
        "id": "LDHVzQUspIUI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45OelKl9KLEn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}